# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import threading
import time
import os
import json
import argparse
import queue
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- Selenium and Anti-Detection Imports ---
try:
    import undetected_chromedriver as uc
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    
# --- Translation ---
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# ==============================================================================
# 0. å…¨å±€é…ç½® (Global Configuration)
# ==============================================================================
# åœ¨ GitHub Actions ä¸­ï¼Œè¿™ä¸ªè¶…æ—¶æ—¶é—´ä¼šè¢«åŠ¨æ€å¢åŠ 
WAIT_TIMEOUT = 120 

def log_message(message):
    """ç®€å•çš„æ—¥å¿—è®°å½•å‡½æ•°ï¼Œæ›¿ä»£GUIæ—¥å¿—"""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")

# ==============================================================================
# 1. æ•°æ®æå–å™¨åŸºç±»å’Œå®ç° (Data Extractor Base & Implementations)
# ==============================================================================

class BaseExtractor:
    """æ‰€æœ‰æå–å™¨çš„åŸºç±»"""
    def __init__(self, log_callback, session=None):
        self.log = log_callback
        self.session = session or requests.Session()
        self.driver = None
        self.wait = None

    def fetch_data(self):
        """
        è¿™ä¸ªæ–¹æ³•åº”è¯¥æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ (generator), 
        ä½¿ç”¨ `yield` è¿”å›æ¯ä¸€ç¯‡æ–‡ç« çš„ä¿¡æ¯å­—å…¸ã€‚
        """
        raise NotImplementedError("æ¯ä¸ªå­ç±»å¿…é¡»å®ç° fetch_data æ–¹æ³•")

    def cleanup(self):
        """æ¸…ç†èµ„æºï¼Œä¾‹å¦‚å…³é—­WebDriver"""
        if self.driver:
            try:
                self.log("æ­£åœ¨å…³é—­ WebDriver...")
                self.driver.quit()
                self.log("âœ… WebDriver å·²å…³é—­ã€‚")
            except Exception as e:
                self.log(f"å…³é—­ WebDriver æ—¶å‡ºé”™: {e}")

    def _setup_driver(self):
        """[å†…éƒ¨æ–¹æ³•] åˆå§‹åŒ– undetected_chromedriverï¼Œå·²ä¸ºæ— å¤´ç¯å¢ƒä¼˜åŒ–ã€‚"""
        if not SELENIUM_AVAILABLE:
            raise ImportError("Selenium æˆ– undetected-chromedriver æœªå®‰è£…ï¼Œæ— æ³•æŠ“å–æ­¤æœŸåˆŠã€‚")
        self.log("ğŸš€ æ­£åœ¨è®¾ç½® undetected-chromedriver for Headless Environment...")
        options = uc.ChromeOptions()
        
        # --- é’ˆå¯¹ GitHub Actions çš„å…³é”®ä¿®æ”¹ ---
        options.add_argument("--headless=new")
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        try:
            chrome_version = 137 
            self.log(f"   - å¼ºåˆ¶ä½¿ç”¨ Chrome v{chrome_version} å¯¹åº”çš„é©±åŠ¨...")
            self.driver = uc.Chrome(options=options, use_subprocess=True, version_main=chrome_version)
            
            # åœ¨CIç¯å¢ƒä¸­å¢åŠ ç­‰å¾…æ—¶é—´
            global WAIT_TIMEOUT
            WAIT_TIMEOUT = 180
            self.wait = WebDriverWait(self.driver, WAIT_TIMEOUT)
            self.log("âœ… undetected-chromedriver åˆå§‹åŒ–æˆåŠŸã€‚")
            return True
        except WebDriverException as e:
            self.log(f"âŒ WebDriver é”™è¯¯: {e}")
            self.log("   - è‡ªåŠ¨åŒ–ç¯å¢ƒä¸­çš„æµè§ˆå™¨åˆå§‹åŒ–å¤±è´¥ã€‚æ£€æŸ¥Chromeç‰ˆæœ¬å’Œé©±åŠ¨è®¾ç½®ã€‚")
            raise ConnectionError(f"æ— æ³•åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨: {e}")
        except Exception as e:
            self.log(f"âŒ åˆå§‹åŒ– undetected-chromedriver å¤±è´¥: {e}")
            raise ConnectionError(f"åˆå§‹åŒ–æµè§ˆå™¨é©±åŠ¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    def _save_debug_info(self):
        """åœ¨æŠ“å–å¤±è´¥æ—¶ä¿å­˜æˆªå›¾å’ŒHTMLæºç """
        if not self.driver:
            return
        
        debug_dir = "debug_output"
        os.makedirs(debug_dir, exist_ok=True)
        
        timestamp = time.strftime('%Y%m%d-%H%M%S')
        
        # ä¿å­˜æˆªå›¾
        screenshot_path = os.path.join(debug_dir, f"error_screenshot_{timestamp}.png")
        try:
            self.driver.save_screenshot(screenshot_path)
            self.log(f"  â„¹ï¸  æˆªå›¾å·²ä¿å­˜åˆ°: {screenshot_path}")
        except Exception as e:
            self.log(f"  âŒ ä¿å­˜æˆªå›¾å¤±è´¥: {e}")

        # ä¿å­˜é¡µé¢HTML
        html_path = os.path.join(debug_dir, f"error_page_{timestamp}.html")
        try:
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(self.driver.page_source)
            self.log(f"  â„¹ï¸  é¡µé¢HTMLå·²ä¿å­˜åˆ°: {html_path}")
        except Exception as e:
            self.log(f"  âŒ ä¿å­˜HTMLå¤±è´¥: {e}")

class AERDataExtractor(BaseExtractor):
    """ä» American Economic Review (AER) ç½‘ç«™æå–è®ºæ–‡ä¿¡æ¯ã€‚"""
    def __init__(self, log_callback, session=None):
        super().__init__(log_callback)
        self.base_url = 'https://www.aeaweb.org'
        self.current_issue_url = f'{self.base_url}/journals/aer/current-issue'

        # åˆ›å»ºä¸€ä¸ªå¸¦æœ‰æ›´é€¼çœŸè¯·æ±‚å¤´çš„ session
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        })
        self.log("AERDataExtractor å·²ä½¿ç”¨å¼ºåŒ–è¯·æ±‚å¤´è¿›è¡Œåˆå§‹åŒ–ã€‚")

    def fetch_data(self):
        self.log("ğŸ” [AER] æ­£åœ¨è·å–æœŸåˆŠä¸»é¡µä»¥æå–æ–‡ç« ID...")
        try:
            self.log(f"   - æ­£åœ¨è®¿é—®: {self.current_issue_url}")
            response = self.session.get(self.current_issue_url, timeout=45)
            response.raise_for_status()
            self.log("   - ä¸»é¡µè®¿é—®æˆåŠŸï¼ŒçŠ¶æ€ç : " + str(response.status_code))
        except requests.exceptions.RequestException as e:
            self.log(f"âŒ è®¿é—®AERæœŸåˆŠä¸»é¡µå¤±è´¥: {e}")
            if 'response' in locals(): self.log(f"   - å“åº”å†…å®¹é¢„è§ˆ: {response.text[:500]}")
            raise ConnectionError(f"æ— æ³•è®¿é—®AERæœŸåˆŠä¸»é¡µ: {e}") from e

        if "Checking if the site connection is secure" in response.text or "human" in response.text.lower():
            self.log("âŒ [AER] æ£€æµ‹åˆ°æœºå™¨äººéªŒè¯é¡µé¢ï¼ŒæŠ“å–å¤±è´¥ã€‚")
            return

        soup = BeautifulSoup(response.content, 'html.parser')
        all_articles = soup.find_all('article', class_='journal-article')
        
        symposia_title = soup.find('article', class_='journal-article symposia-title')
        target_articles = all_articles
        if symposia_title:
            try:
                symposia_index = all_articles.index(symposia_title)
                target_articles = all_articles[symposia_index + 1:]
            except ValueError:
                self.log("âš ï¸ [AER] æœªæ‰¾åˆ° symposia-title çš„ç¡®åˆ‡ä½ç½®ï¼Œå°†å°è¯•æŠ“å–æ‰€æœ‰æ–‡ç« ã€‚")

        article_ids = [a.get('id') for a in target_articles if a.get('id') and 'symposia-title' not in a.get('class', [])]
        self.log(f"âœ… [AER] æ‰¾åˆ° {len(article_ids)} ç¯‡å¾…å¤„ç†æ–‡ç« ã€‚")

        if not article_ids:
            return

        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_id = {executor.submit(self._get_single_article_details, aid): aid for aid in article_ids}
            for future in as_completed(future_to_id):
                result = future.result()
                if result:
                    yield result

    def _get_single_article_details(self, article_id: str):
        article_url = f'{self.base_url}/articles?id={article_id}'
        try:
            response = self.session.get(article_url, timeout=45)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            title = soup.find(class_='title').get_text(strip=True) if soup.find(class_='title') else 'Title not found'
            authors = ", ".join([a.get_text(strip=True) for a in soup.select('.attribution .author')]) or 'Authors not found'
            abstract_element = soup.find('section', class_='article-information abstract')
            abstract = 'Abstract not found'
            if abstract_element:
                raw_text = abstract_element.get_text(strip=True)
                abstract = ' '.join((raw_text[8:] if raw_text.lower().startswith('abstract') else raw_text).split())
            return {'url': article_url, 'title': title, 'authors': authors, 'abstract': abstract}
        except requests.exceptions.RequestException as e:
            self.log(f"  âŒ æŠ“å–AERæ–‡ç«  {article_id} å¤±è´¥: {e}")
            return None

class JPEDataExtractor(BaseExtractor):
    """ä» Journal of Political Economy (JPE) ç½‘ç«™æå–è®ºæ–‡ä¿¡æ¯ã€‚"""
    def fetch_data(self):
        self._setup_driver()
        base_url = "https://www.journals.uchicago.edu"
        current_issue_url = f"{base_url}/toc/jpe/current"
        self.log(f"æ­£åœ¨å¯¼èˆªè‡³JPEä¸»é¡µ: {current_issue_url}")
        self.driver.get(current_issue_url)

        try:
            self.log(f"â³ æ­£åœ¨ç­‰å¾…JPEæ–‡ç« åˆ—è¡¨åŠ è½½...")
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "li.form-action-item")))
            list_items = self.driver.find_elements(By.CSS_SELECTOR, "li.form-action-item")
            urls = []
            for item in list_items:
                try:
                    item.find_element(By.CSS_SELECTOR, ".issue-item__loa")
                    link_element = item.find_element(By.CSS_SELECTOR, ".issue-item__title a")
                    href = link_element.get_attribute('href')
                    urls.append(f"{base_url}{href}" if href.startswith('/') else href)
                except NoSuchElementException:
                    continue
            
            self.log(f"âœ… æ‰¾åˆ° {len(urls)} ç¯‡JPEæ–‡ç« ï¼Œå¼€å§‹é€ä¸€æŠ“å–è¯¦æƒ…...")
            for url in urls:
                details = self._get_single_article_details(url)
                if details:
                    yield details
        
        except TimeoutException:
            self.log(f"âŒ é”™è¯¯: åœ¨ {WAIT_TIMEOUT} ç§’å†…æœªèƒ½åŠ è½½JPEæ–‡ç« åˆ—è¡¨ã€‚")
            self._save_debug_info() # ä¿å­˜è°ƒè¯•ä¿¡æ¯
        finally:
            self.cleanup()

    def _get_single_article_details(self, article_url):
        self.log(f"  > æ­£åœ¨è®¿é—®JPEæ–‡ç« : {article_url.split('/')[-1]}")
        try:
            self.driver.get(article_url)
            title = self.wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "h1.citation__title"))).text.strip()
            author_container = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".accordion-tabbed.loa-accordion")))
            authors = ", ".join([span.text.strip() for span in author_container.find_elements(By.CSS_SELECTOR, "a.author-name span")]) or "ä½œè€…æœªæ‰¾åˆ°"
            abstract = self.wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, ".abstractSection.abstractInFull p"))).text.strip()
            return {'url': article_url, 'title': title, 'authors': authors, 'abstract': abstract}
        except Exception as e:
            self.log(f"  âŒ æŠ“å–JPEæ–‡ç«  {article_url} è¯¦æƒ…å¤±è´¥: {e}")
            self._save_debug_info() # ä¿å­˜è°ƒè¯•ä¿¡æ¯
            return None

class OUPBaseExtractor(BaseExtractor):
    """ä¸ºOUPå¹³å°(QJE, RES)è®¾è®¡çš„åŸºç±»æå–å™¨"""
    journal_code = "" # e.g., "qje", "restud"
    
    def fetch_data(self):
        self._setup_driver()
        base_url = "https://academic.oup.com"
        current_issue_url = f"{base_url}/{self.journal_code}/issue"
        self.log(f"æ­£åœ¨å¯¼èˆªè‡³ {self.journal_code.upper()} ä¸»é¡µ: {current_issue_url}")
        self.driver.get(current_issue_url)

        try:
            self.log(f"â³ æ­£åœ¨ç­‰å¾… {self.journal_code.upper()} æ–‡ç« åˆ—è¡¨åŠ è½½...")
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".al-article-item-wrap.al-normal")))
            list_items = self.driver.find_elements(By.CSS_SELECTOR, ".al-article-item-wrap.al-normal")
            urls = []
            for item in list_items:
                try:
                    item.find_element(By.CSS_SELECTOR, ".al-authors-list")
                    link_element = item.find_element(By.CSS_SELECTOR, "h5.item-title a.at-articleLink")
                    href = link_element.get_attribute('href')
                    urls.append(href if href.startswith('http') else f"{base_url}{href}")
                except NoSuchElementException:
                    continue
            
            self.log(f"âœ… æ‰¾åˆ° {len(urls)} ç¯‡ {self.journal_code.upper()} æ–‡ç« ï¼Œå¼€å§‹é€ä¸€æŠ“å–è¯¦æƒ…...")
            for url in urls:
                details = self._get_single_article_details(url)
                if details:
                    yield details
        
        except TimeoutException:
            self.log(f"âŒ é”™è¯¯: åœ¨ {WAIT_TIMEOUT} ç§’å†…æœªèƒ½åŠ è½½ {self.journal_code.upper()} æ–‡ç« åˆ—è¡¨ã€‚")
            self._save_debug_info()
        finally:
            self.cleanup()

    def _get_single_article_details(self, article_url):
        self.log(f"  > æ­£åœ¨è®¿é—® {self.journal_code.upper()} æ–‡ç« : {article_url.split('/')[-1]}")
        try:
            self.driver.get(article_url)
            title = self.wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "h1.wi-article-title"))).text.strip()
            authors_container = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".al-authors-list")))
            author_elements = authors_container.find_elements(By.CSS_SELECTOR, ".al-author-name-role > a")
            if not author_elements: author_elements = authors_container.find_elements(By.TAG_NAME, "a")
            authors_list = [elem.text.strip().strip(',') for elem in author_elements]
            authors = ", ".join([name for name in authors_list if name]) or "ä½œè€…æœªæ‰¾åˆ°"
            abstract_container = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "section.abstract")))
            abstract = abstract_container.find_element(By.CSS_SELECTOR, "p.chapter-para").text.strip()
            return {'url': article_url, 'title': title, 'authors': authors, 'abstract': abstract}
        except Exception as e:
            self.log(f"  âŒ æŠ“å– {self.journal_code.upper()} æ–‡ç«  {article_url} è¯¦æƒ…å¤±è´¥: {e}")
            self._save_debug_info()
            return None

class QJEDataExtractor(OUPBaseExtractor):
    journal_code = "qje"

class RESDataExtractor(OUPBaseExtractor):
    journal_code = "restud"

class ECTADataExtractor(BaseExtractor):
    """ä» Econometrica (ECTA) ç½‘ç«™æå–è®ºæ–‡ä¿¡æ¯ã€‚"""
    def fetch_data(self):
        self._setup_driver()
        base_url = "https://onlinelibrary.wiley.com"
        current_issue_url = f"{base_url}/toc/14680262/current"
        self.log(f"æ­£åœ¨å¯¼èˆªè‡³ECTAä¸»é¡µ: {current_issue_url}")
        self.driver.get(current_issue_url)

        try:
            self.log("â³ æ­£åœ¨ç­‰å¾…ECTAæ–‡ç« åˆ—è¡¨åŠ è½½...")
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.issue-item")))
            list_items = self.driver.find_elements(By.CSS_SELECTOR, "div.issue-item")
            urls = []
            for item in list_items:
                try:
                    link_element = item.find_element(By.CSS_SELECTOR, "a.issue-item__title.visitable")
                    href = link_element.get_attribute('href')
                    doi_part = href.split('/')[-1]
                    if doi_part and doi_part[-1].isdigit():
                        urls.append(f"{base_url}{href}" if href.startswith('/') else href)
                except NoSuchElementException:
                    continue
            
            self.log(f"âœ… æ‰¾åˆ° {len(urls)} ç¯‡ECTAæ–‡ç« ï¼Œå¼€å§‹é€ä¸€æŠ“å–è¯¦æƒ…...")
            for url in urls:
                details = self._get_single_article_details(url)
                if details:
                    yield details

        except TimeoutException:
            self.log(f"âŒ é”™è¯¯: åœ¨ {WAIT_TIMEOUT} ç§’å†…æœªèƒ½åŠ è½½ECTAæ–‡ç« åˆ—è¡¨ã€‚")
            self._save_debug_info()
        finally:
            self.cleanup()

    def _get_single_article_details(self, article_url):
        self.log(f"  > æ­£åœ¨è®¿é—®ECTAæ–‡ç« : {article_url.split('/')[-1]}")
        try:
            self.driver.get(article_url)
            title = self.wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, "h1.citation__title"))).text.strip()
            author_container = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.accordion-tabbed")))
            authors = ", ".join([span.text.strip() for span in author_container.find_elements(By.CSS_SELECTOR, "a.author-name > span")]) or "ä½œè€…æœªæ‰¾åˆ°"
            abstract_container = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "div.article-section__content.en.main")))
            abstract = "\n\n".join([p.text.strip() for p in abstract_container.find_elements(By.TAG_NAME, "p")]) or "æ‘˜è¦æœªæ‰¾åˆ°"
            return {'url': article_url, 'title': title, 'authors': authors, 'abstract': abstract}
        except Exception as e:
            self.log(f"  âŒ æŠ“å–ECTAæ–‡ç«  {article_url} è¯¦æƒ…å¤±è´¥: {e}")
            self._save_debug_info()
            return None

# ==============================================================================
# 2. ç¿»è¯‘ä¸ä¸»é€»è¾‘ (Translation & Main Logic)
# ==============================================================================

def translate_with_kimi(text, kimi_client):
    if not text or "not found" in text.lower():
        return "å†…å®¹ç¼ºå¤±"
    if not kimi_client:
        log_message("è­¦å‘Š: Kimi å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ç¿»è¯‘ã€‚")
        return "(æœªç¿»è¯‘)"
    try:
        response = kimi_client.chat.completions.create(
            model="moonshot-v1-8k",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é¢†åŸŸç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„è‹±æ–‡æ–‡æœ¬å‡†ç¡®ã€æµç•…åœ°ç¿»è¯‘æˆä¸­æ–‡ã€‚è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–è¯´æ˜æˆ–å®¢å¥—è¯ã€‚"},
                {"role": "user", "content": text}
            ],
            temperature=0.3, max_tokens=2000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        log_message(f"Kimiç¿»è¯‘å¤±è´¥: {str(e)[:100]}...")
        return f"ç¿»è¯‘å¤±è´¥: {e}"

def process_journal(journal_key, kimi_client):
    """å¤„ç†å•ä¸ªæœŸåˆŠçš„æŠ“å–å’Œç¿»è¯‘"""
    extractors = {
        "AER": AERDataExtractor, "JPE": JPEDataExtractor,
        "QJE": QJEDataExtractor, "RES": RESDataExtractor,
        "ECTA": ECTADataExtractor,
    }
    
    full_journal_names = {
        "AER": "American Economic Review", "JPE": "Journal of Political Economy",
        "QJE": "The Quarterly Journal of Economics", "RES": "The Review of Economic Studies",
        "ECTA": "Econometrica",
    }

    if journal_key not in extractors:
        log_message(f"é”™è¯¯: æœªçŸ¥çš„æœŸåˆŠä»£ç  '{journal_key}'")
        return

    log_message(f"--- å¼€å§‹å¤„ç†: {journal_key} ---")
    extractor_class = extractors[journal_key]
    extractor = extractor_class(log_callback=log_message)
    
    processed_articles = []
    
    try:
        raw_articles = list(extractor.fetch_data())
        log_message(f"æ‰¾åˆ° {len(raw_articles)} ç¯‡æ¥è‡ª {journal_key} çš„æ–‡ç« ã€‚")

        if not raw_articles:
            log_message(f"åœ¨ {journal_key} æœªæ‰¾åˆ°æ–‡ç« ï¼Œå¤„ç†ç»“æŸã€‚")
        else:
            with ThreadPoolExecutor(max_workers=5) as executor:
                # æäº¤æ‰€æœ‰ç¿»è¯‘ä»»åŠ¡
                future_to_article = {
                    executor.submit(translate_with_kimi, article['abstract'], kimi_client): (article, 'abstract_cn')
                    for article in raw_articles
                }
                future_to_article.update({
                    executor.submit(translate_with_kimi, article['title'], kimi_client): (article, 'title_cn')
                    for article in raw_articles
                })

                # è·å–ç¿»è¯‘ç»“æœå¹¶æ›´æ–°æ–‡ç« å­—å…¸
                for future in as_completed(future_to_article):
                    article, key = future_to_article[future]
                    try:
                        translation = future.result()
                        article[key] = translation
                    except Exception as exc:
                        log_message(f"ç¿»è¯‘ä»»åŠ¡å¤±è´¥: {exc}")
                        article[key] = "ç¿»è¯‘å‡ºé”™"
            
            # æ•´ç†ç»“æœ
            processed_articles = raw_articles
        
        # å‡†å¤‡æœ€ç»ˆçš„JSONè¾“å‡º
        output_data = {
            "journal_key": journal_key,
            "journal_full_name": full_journal_names.get(journal_key, journal_key),
            "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),
            "articles": processed_articles
        }
        
    except Exception as e:
        log_message(f"âŒ å¤„ç† {journal_key} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        output_data = {
            "journal_key": journal_key,
            "journal_full_name": full_journal_names.get(journal_key, journal_key),
            "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),
            "error": str(e),
            "articles": []
        }
    finally:
        extractor.cleanup()

    # å°†ç»“æœå†™å…¥JSONæ–‡ä»¶
    output_filename = f"{journal_key}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=4)
    log_message(f"âœ… å·²å°† {journal_key} çš„æ•°æ®å†™å…¥åˆ° {output_filename}")


# ==============================================================================
# 3. ç¨‹åºå…¥å£ (Application Entry Point)
# ==============================================================================
def main():
    parser = argparse.ArgumentParser(description="æŠ“å–ç»æµå­¦æœŸåˆŠæœ€æ–°è®ºæ–‡ã€‚")
    parser.add_argument("journal", help="è¦æŠ“å–çš„æœŸåˆŠä»£ç  (e.g., AER, JPE, ALL)ã€‚")
    args = parser.parse_args()

    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–APIå¯†é’¥
    kimi_api_key = os.getenv('KIMI_API_KEY')
    kimi_client = None
    
    if OPENAI_AVAILABLE and kimi_api_key:
        try:
            kimi_client = OpenAI(api_key=kimi_api_key, base_url="https://api.moonshot.cn/v1")
            log_message("Kimi API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception as e:
            log_message(f"åˆå§‹åŒ–Kimiå®¢æˆ·ç«¯å¤±è´¥: {e}")
    else:
        log_message("KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ– openai åº“ä¸å¯ç”¨ï¼Œå°†ä¸è¿›è¡Œç¿»è¯‘ã€‚")

    journals_to_process = ["AER", "JPE", "QJE", "RES", "ECTA"]
    
    if args.journal.upper() == 'ALL':
        # åœ¨CIç¯å¢ƒä¸­ï¼Œæˆ‘ä»¬é€šè¿‡çŸ©é˜µç­–ç•¥å¹¶è¡Œè¿è¡Œï¼Œæ‰€ä»¥ä¸éœ€è¦è¿™ä¸ªåˆ†æ”¯
        # ä½†æœ¬åœ°æµ‹è¯•æ—¶ä»ç„¶æœ‰ç”¨
        for journal in journals_to_process:
            process_journal(journal, kimi_client)
    elif args.journal.upper() in journals_to_process:
        process_journal(args.journal.upper(), kimi_client)
    else:
        log_message(f"é”™è¯¯: ä¸æ”¯æŒçš„æœŸåˆŠä»£ç  '{args.journal}'. è¯·ä½¿ç”¨ 'ALL' æˆ–ä»¥ä¸‹ä¹‹ä¸€: {', '.join(journals_to_process)}")

if __name__ == "__main__":
    main()
