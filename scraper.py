# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import time
import os
import json
import argparse
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# --- ç¿»è¯‘æ¨¡å— ---
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    OpenAI = None # å®šä¹‰ä¸€ä¸ªå ä½ç¬¦ï¼Œé˜²æ­¢æœªå®‰è£…æ—¶æŠ¥é”™

# ==============================================================================
# 0. å…¨å±€é…ç½®ä¸å·¥å…·å‡½æ•°
# ==============================================================================

def log_message(message):
    """é€šç”¨æ—¥å¿—è®°å½•å‡½æ•°"""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}")

# åˆ›å»ºä¸€ä¸ªå…±äº«çš„ã€å¸¦æœ‰é€šç”¨æµè§ˆå™¨å¤´çš„ requests.Session
# è¿™ä¸ª session ä¼šè¢«æ‰€æœ‰åŸºäº requests çš„æå–å™¨ä½¿ç”¨
session = requests.Session()
session.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'Accept-Language': 'en-US,en;q=0.9',
})

# ==============================================================================
# 1. æ•°æ®æå–å™¨ç±»
# ==============================================================================

# --- AER ä¸“ç”¨æå–å™¨ (ç›´æ¥æŠ“å–å®˜ç½‘) ---
class AERDataExtractor:
    def __init__(self):
        self.base_url = 'https://www.aeaweb.org'
        self.current_issue_url = f'{self.base_url}/journals/aer/current-issue'

    def get_article_ids(self):
        log_message("ğŸ” [AER] æ­£åœ¨è·å–æœŸåˆŠä¸»é¡µä»¥æå–æ–‡ç«  ID...")
        response = session.get(self.current_issue_url, timeout=30)
        response.raise_for_status()
        if "Checking if the site connection is secure" in response.text:
            raise ConnectionError("[AER] è¢«æœºå™¨äººéªŒè¯æ‹¦æˆªï¼Œæ— æ³•ç»§ç»­ã€‚")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        all_articles = soup.find_all('article', class_='journal-article')
        symposia_title = soup.find('article', class_='journal-article symposia-title')
        target_articles = all_articles
        if symposia_title:
            try:
                symposia_index = all_articles.index(symposia_title)
                target_articles = all_articles[symposia_index + 1:]
            except ValueError:
                pass
        
        article_ids = [a.get('id') for a in target_articles if 'symposia-title' not in a.get('class', []) and a.get('id')]
        log_message(f"âœ… [AER] æ‰¾åˆ° {len(article_ids)} ç¯‡æ–‡ç« å¾…å¤„ç†ã€‚")
        return article_ids

    def get_single_article_details(self, article_id: str):
        log_message(f"  > [AER] æ­£åœ¨æŠ“å–æ–‡ç« è¯¦æƒ…: {article_id}")
        article_url = f'{self.base_url}/articles?id={article_id}'
        response = session.get(article_url, timeout=30)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find(class_='title').get_text(strip=True) if soup.find(class_='title') else 'Title not found'
        authors = ", ".join([a.get_text(strip=True) for a in soup.select('.attribution .author')]) or 'Authors not found'
        abstract_element = soup.find('section', class_='article-information abstract')
        abstract = 'Abstract not found'
        if abstract_element:
            raw_text = abstract_element.get_text(strip=True)
            abstract = ' '.join((raw_text[8:] if raw_text.lower().startswith('abstract') else raw_text).split())
        
        return {'url': article_url, 'title': title, 'authors': authors, 'abstract': abstract}

# --- åŸºäº RSS çš„æå–å™¨ ---
class BaseRssExtractor:
    def __init__(self, journal_name, rss_url):
        self.journal_name = journal_name
        self.rss_url = rss_url

    def _get_soup(self, url, parser='xml'):
        try:
            response = session.get(url, timeout=30)
            response.raise_for_status()
            return BeautifulSoup(response.content, parser)
        except requests.RequestException as e:
            log_message(f"âŒ [{self.journal_name}] è¯·æ±‚å¤±è´¥: {url}: {e}")
            return None

class EctaRssExtractor(BaseRssExtractor):
    def fetch_articles(self):
        log_message(f"ğŸ” [{self.journal_name}] æ­£åœ¨ä» RSS Feed è·å–æ–‡ç« ...")
        soup = self._get_soup(self.rss_url)
        if not soup: return [], None
        
        articles = []
        volume, issue = None, None
        for item in soup.find_all('item'):
            abstract_html = item.find('content:encoded').text.strip()
            articles.append({
                'url': item.link.text.strip(),
                'title': item.title.text.strip(),
                'authors': item.find('dc:creator').text.strip() if item.find('dc:creator') else "ä½œè€…æœªæ‰¾åˆ°",
                'abstract': BeautifulSoup(abstract_html, 'html.parser').get_text().strip()
            })
            if not volume and item.find('prism:volume'): volume = item.find('prism:volume').text.strip()
            if not issue and item.find('prism:number'): issue = item.find('prism:number').text.strip()
        
        report_header = f"{datetime.now().year}å¹´ ç¬¬{volume}å·(Vol. {volume}) ç¬¬{issue}æœŸ" if volume and issue else None
        return articles, report_header

class TwoStageRssExtractor(BaseRssExtractor):
    def fetch_articles(self):
        log_message(f"ğŸ” [{self.journal_name}] é˜¶æ®µ1: ä» RSS è·å–é“¾æ¥...")
        rss_soup = self._get_soup(self.rss_url)
        if not rss_soup: return [], None
        
        article_links = self._parse_rss_for_links(rss_soup)
        log_message(f"âœ… [{self.journal_name}] é˜¶æ®µ1: æ‰¾åˆ° {len(article_links)} ä¸ªæœ‰æ•ˆé“¾æ¥ã€‚")
        if not article_links: return [], None

        log_message(f"ğŸ” [{self.journal_name}] é˜¶æ®µ2: å¹¶è¡ŒæŠ“å–è¯¦æƒ…é¡µ...")
        articles = []
        with ThreadPoolExecutor(max_workers=5) as executor:
            future_to_url = {executor.submit(self._scrape_detail_page, url): url for url in article_links}
            for future in as_completed(future_to_url):
                if result := future.result():
                    articles.append(result)
        
        report_header = None
        if articles:
            first_article = articles[0]
            vol, iss = first_article.get('volume'), first_article.get('issue')
            if vol and iss: report_header = f"{datetime.now().year}å¹´ ç¬¬{vol}å·(Vol. {vol}) ç¬¬{iss}æœŸ"
        
        return articles, report_header

    def _parse_rss_for_links(self, soup): raise NotImplementedError
    def _scrape_detail_page(self, url): raise NotImplementedError

class JpeRssExtractor(TwoStageRssExtractor):
    def _parse_rss_for_links(self, soup):
        return [item.get('rdf:about') for item in soup.find_all('item') if item.find('dc:creator') and item.get('rdf:about')]

    def _scrape_detail_page(self, url):
        soup = self._get_soup(url, parser='html.parser')
        if not soup: return None
        try:
            volume_text = soup.select_one('.issue-info-pub-date a').text.strip() if soup.select_one('.issue-info-pub-date a') else ""
            vol = iss = None
            if "Volume" in volume_text and "Issue" in volume_text:
                parts = volume_text.split(',')
                vol = parts[0].replace("Volume", "").strip()
                iss = parts[1].replace("Issue", "").strip()
            return {
                'url': url,
                'title': soup.find('h1', class_='citation__title').text.strip(),
                'authors': ", ".join([a.text.strip() for a in soup.select('.author-name span')]) or "ä½œè€…æœªæ‰¾åˆ°",
                'abstract': soup.find('div', class_='abstractSection').p.text.strip(),
                'volume': vol, 'issue': iss
            }
        except AttributeError as e:
            log_message(f"  âŒ [{self.journal_name}] è§£æè¯¦æƒ…é¡µå¤±è´¥ {url.split('?')[0]}: {e}")
            return None

class OupRssExtractor(TwoStageRssExtractor):
    def _parse_rss_for_links(self, soup):
        return [item.link.text.strip() for item in soup.find_all('item')]

    def _scrape_detail_page(self, url):
        soup = self._get_soup(url, parser='html.parser')
        if not soup: return None
        try:
            issue_info = soup.select_one('.issue-info')
            return {
                'url': url,
                'title': soup.find('h1', class_='wi-article-title').text.strip(),
                'authors': ", ".join([a.text.strip().strip(',') for a in soup.select('.wi-authors a.linked-name')]) or "ä½œè€…æœªæ‰¾åˆ°",
                'abstract': soup.find('section', class_='abstract').p.text.strip(),
                'volume': issue_info['data-vol'] if issue_info and 'data-vol' in issue_info.attrs else None,
                'issue': issue_info['data-issue'] if issue_info and 'data-issue' in issue_info.attrs else None
            }
        except AttributeError as e:
            log_message(f"  âŒ [{self.journal_name}] è§£æè¯¦æƒ…é¡µå¤±è´¥ {url.split('?')[0]}: {e}")
            return None

# ==============================================================================
# 2. æ ¸å¿ƒå¤„ç†é€»è¾‘
# ==============================================================================
def translate_with_kimi(text, kimi_client):
    if not text or "not found" in text.lower(): return "å†…å®¹ç¼ºå¤±"
    if not kimi_client: return "(æœªç¿»è¯‘)"
    try:
        response = kimi_client.chat.completions.create(
            model="moonshot-v1-8k",
            messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é¢†åŸŸç¿»è¯‘åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·æä¾›çš„è‹±æ–‡æ–‡æœ¬å‡†ç¡®ã€æµç•…åœ°ç¿»è¯‘æˆä¸­æ–‡ã€‚è¯·ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–è¯´æ˜æˆ–å®¢å¥—è¯ã€‚"},
                      {"role": "user", "content": text}],
            temperature=0.3, max_tokens=2000
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        log_message(f"Kimiç¿»è¯‘å¤±è´¥: {str(e)[:100]}...")
        return f"ç¿»è¯‘å¤±è´¥: {e}"

def process_journal(journal_key, kimi_client):
    """ä¸»æµç¨‹ï¼šæ ¹æ®æœŸåˆŠä»£ç é€‰æ‹©ä¸åŒç­–ç•¥è¿›è¡Œå¤„ç†"""
    
    full_journal_names = {
        "AER": "American Economic Review", "JPE": "Journal of Political Economy",
        "QJE": "The Quarterly Journal of Economics", "RES": "The Review of Economic Studies",
        "ECTA": "Econometrica",
    }
    
    log_message(f"--- å¼€å§‹å¤„ç†: {journal_key} ---")
    
    extractor_map = {
        "JPE": JpeRssExtractor("JPE", "https://www.journals.uchicago.edu/action/showFeed?ui=0&mi=0&ai=t6&jc=jpe&type=etoc&feed=rss"),
        "QJE": OupRssExtractor("QJE", "https://academic.oup.com/rss/site_5504/3365.xml"),
        "RES": OupRssExtractor("RES", "https://academic.oup.com/rss/site_5508/3369.xml"),
        "ECTA": EctaRssExtractor("ECTA", "https://onlinelibrary.wiley.com/feed/14680262/most-recent"),
    }
    
    try:
        # AER ä½¿ç”¨ç‰¹æ®Šå¤„ç†æµç¨‹
        if journal_key == "AER":
            aer_extractor = AERDataExtractor()
            article_ids = aer_extractor.get_article_ids()
            raw_articles = []
            if article_ids:
                with ThreadPoolExecutor(max_workers=5) as executor:
                    future_to_id = {executor.submit(aer_extractor.get_single_article_details, aid): aid for aid in article_ids}
                    for future in as_completed(future_to_id):
                        if result := future.result():
                            raw_articles.append(result)
            report_header = None # AERä¸æ–¹ä¾¿ä»é¡µé¢æå–å·æœŸï¼Œæ•…ä¸ç”Ÿæˆ
        else: # å…¶ä»–æœŸåˆŠä½¿ç”¨RSSæµç¨‹
            extractor = extractor_map[journal_key]
            raw_articles, report_header = extractor.fetch_articles()

        log_message(f"âœ… æ‰¾åˆ° {len(raw_articles)} ç¯‡æ¥è‡ª {journal_key} çš„æœ‰æ•ˆæ–‡ç« ã€‚")
        
        processed_articles = []
        if raw_articles:
            with ThreadPoolExecutor(max_workers=8) as executor: # å¢åŠ ç¿»è¯‘çº¿ç¨‹
                for article in raw_articles:
                    article['title_cn_future'] = executor.submit(translate_with_kimi, article['title'], kimi_client)
                    article['abstract_cn_future'] = executor.submit(translate_with_kimi, article['abstract'], kimi_client)

            for article in raw_articles:
                article['title_cn'] = article.pop('title_cn_future').result()
                article['abstract_cn'] = article.pop('abstract_cn_future').result()
                processed_articles.append(article)
        
        output_data = {
            "journal_key": journal_key,
            "journal_full_name": full_journal_names[journal_key],
            "report_header": report_header or f"{datetime.now().year}å¹´ æœ€æ–°ä¸€æœŸ",
            "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),
            "articles": processed_articles
        }
        
    except Exception as e:
        log_message(f"âŒ å¤„ç† {journal_key} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        output_data = {
            "journal_key": journal_key,
            "journal_full_name": full_journal_names.get(journal_key, journal_key),
            "report_header": f"{datetime.now().year}å¹´ æ•°æ®è·å–å¤±è´¥",
            "update_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC'),
            "error": str(e), "articles": []
        }
    
    # ç»Ÿä¸€å†™å…¥JSONæ–‡ä»¶
    output_filename = f"{journal_key}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=4)
    log_message(f"âœ… å·²å°† {journal_key} çš„æ•°æ®å†™å…¥åˆ° {output_filename}")

# ==============================================================================
# 3. ç¨‹åºå…¥å£
# ==============================================================================
def main():
    parser = argparse.ArgumentParser(description="é€šè¿‡æ··åˆç­–ç•¥æŠ“å–ç»æµå­¦æœŸåˆŠæœ€æ–°è®ºæ–‡ã€‚")
    parser.add_argument("journal", help="è¦æŠ“å–çš„æœŸåˆŠä»£ç  (e.g., AER, JPE, ALL)ã€‚")
    args = parser.parse_args()

    kimi_api_key = os.getenv('KIMI_API_KEY')
    kimi_client = None
    if OPENAI_AVAILABLE and kimi_api_key:
        try:
            kimi_client = OpenAI(api_key=kimi_api_key, base_url="https://api.moonshot.cn/v1")
            log_message("Kimi API å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception as e:
            log_message(f"åˆå§‹åŒ–Kimiå®¢æˆ·ç«¯å¤±è´¥: {e}")
    else:
        log_message("KIMI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®æˆ– openai åº“ä¸å¯ç”¨ï¼Œå°†ä¸è¿›è¡Œç¿»è¯‘ã€‚")

    if args.journal.upper() in ["AER", "JPE", "QJE", "RES", "ECTA"]:
        process_journal(args.journal.upper(), kimi_client)
    else:
        log_message(f"é”™è¯¯: ä¸æ”¯æŒçš„æœŸåˆŠä»£ç  '{args.journal}'. GitHub Actions ä¼šä¸ºæ¯ä¸ªæœŸåˆŠå•ç‹¬è¿è¡Œæ­¤è„šæœ¬ã€‚")

if __name__ == "__main__":
    main()
